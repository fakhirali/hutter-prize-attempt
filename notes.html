<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>My Notes</title>
    <style>
        body {
            background-color: #121212;
            color: #e0e0e0;
            font-family: Arial, sans-serif;
            line-height: 1.6;
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
        }
        pre {
            background-color: #1e1e1e;
            color: #e0e0e0;
            padding: 10px;
            border-radius: 5px;
            overflow-x: auto;
        }
        code {
            color: #e0e0e0;
            font-family: monospace;
        }
        a {
            color: #bb86fc; /* Light purple for visibility on dark bg */
        }
        a:hover {
            text-decoration: underline;
        }
    </style>
</head>
<body>
    <h1>Notes</h1>
<p>I'll try to implement stuff in tinygrad so that it can be converted into a c program later.
Will have to see how that works. If not, I'll just write it in c.</p>
<p><a href="http://prize.hutter1.net/hfaq.htm#specomp">Here</a> it says that the top compressors
would have components that apply to language modeling or learning in general instead of
it being specific to enwik9. There needs to be a clear understanding of which stuff
is transferable.</p>
<p>Is there any practical use of these compressors? Can one create a useful language model
from them? Can they be run on a gpu? Can we train on the internet using these compressors?
There has to be an attempt to make practical use of these compressors or the techniques
present in them. There needs to be a bridge between this and the overall NLP field.</p>
<p><a href="https://arxiv.org/pdf/2306.13812">Continual learning</a> from rich sutton. Is a useful idea here.
Since the learning would have to be online. <a href="https://openreview.net/pdf?id=Hygi7xStvS">This</a> paper
also includes useful techniques like revisiting past data. The paper uses enwik8. The FAQ <a href="http://prize.hutter1.net/hfaq.htm#largenn">here</a>
says that NNs are too slow for enwik9 for now.</p>
<p>Intuitively I feel that NNs should be the solution to the hutter prize. We just gotta make them
run fast and learn quickly. Techniques that allow the NNs to learn quickly and run fast is what we want
to go for.</p>
<p>In <a href="https://www.byronknoll.com/cmix.html">cmix</a>, there is a dictionary for enwik9. Would that be
helpful for an NN as well. Is the dictionary the ideal way to have maximum compression of the symbols?
Would <a href="https://huggingface.co/learn/llm-course/en/chapter6/5">BPE</a> or something work here?</p>
<p>Techniques that help NNs, run fast and learn quickly. Continual learning is one of them.
Pruing, Quantization help too. Learning rate tricks. Should look into <a href="https://github.com/karpathy/llama2.c">llama2.c</a>.</p>
<p>NNs require more data to learn better. A way to improve an NN's performance is to get more and better data.
Here we don't really have more data.</p>

</body>
</html>